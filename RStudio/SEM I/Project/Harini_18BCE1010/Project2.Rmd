---
title: "New York AirBnb dataset"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## R. Harini

## 18BCE1010

### About the Dataset: Airbnb Dataset from insideairbnb.com
#### For a decade now, people have used Airbnb to expand travelling possibilities and present a personalized way of exploring the world. 
#### With the current scenario of COVID-19, hotels all around the world are a facing a decline in their profits. 
#### Due to this, extracting important information from the previously provided data becomes extremely crucial in order to enhance the business.
#### The dataset depicts the listing activity and metrics in New York for 2019.

### Exploration Possibilities: 
#### Learning about different hosts and areas
#### Learning from the trends in prices and reviews
#### Which location is the busiest and has the maximum number of reviews?
#### Which group generates maximum revenue?
#### Visualizing the distribution of the income 
#### Frequency distribution of neighborhood and room types

```{r setup, include=FALSE}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(reactable)
library(formattable)
library(leaps)
library(caret)
library(tidyverse)
library(lattice)
library(e1071)
library(caTools)
library(car)

```


```{r}
data=read.csv("AB_NYC_2019.csv", stringsAsFactors = TRUE)

```

#### Describing the dataset
```{r}
summary(data)
str(data)
dim(data)
colnames(data)

```

## Cleaning the dataset

#### ID column
```{r}

class(data$id)
sum(is.na(data$id))
```

#### Name column
```{r}

class(data$name)
data$name=as.character(data$name)
class(data$name)
```


#### Host ID
```{r}
class(data$host_id)
sum(is.na(data$host_id))
```

#### Dropping columns that are insignificant for data exploration
```{r}
data=subset(data,select=-c(id,host_name,last_review))
```



#### Replacing Na values in reviews_per_month with 0
```{r}
sum(is.na(data$reviews_per_month))
data=data%>%mutate(reviews_per_month=replace(reviews_per_month,is.na(reviews_per_month),0))
sum(is.na(data$reviews_per_month))
```

#### Removing all the rows with price==0 as it is erroneous data
```{r}
nrow(data[which(data$price==0),])
data=data[!data$price==0,]
nrow(data[which(data$price==0),])
```


#### Different Neighbourhood group
```{r}
levels(data$neighbourhood_group)
```

#### Different Room types
```{r}
levels(data$room_type)
```


#### Checking which Host IDs have the highest listings
```{r}

top=data%>%
  group_by(host_id)%>%
  tally()
top
top=top[order(-top$n),]
top=head(top)
top
barplot(top$n, names.arg = top$host_id, xlab = "HOST IDS", ylab="LISTINGS", col="light blue")
```

#### Visualising the distribution of Price
```{r}
ggplot(data, aes(x=price))+ geom_histogram(fill="cyan",color="black", bins=70, size=1.5)+theme_minimal()

d=data%>%
  filter(price<1000)

ggplot(d, aes(x=price))+ geom_histogram(fill="cyan",color="black", bins=70, size=1.5)+theme_minimal()
```

##### We can see that the most sought after price range is 0-250$

#### Visualising the distribution of Minimum Nights
```{r}
ggplot(data, aes(x=minimum_nights))+ geom_histogram(fill="green",color="black", bins=70, size=1.5)+theme_minimal()

d=data%>%
  filter(minimum_nights<50)
ggplot(d, aes(x=minimum_nights))+ geom_histogram(fill="green",color="black", bins=70, size=1.5)+theme_minimal()
```

#### Finding the top 5 frequent Neighbourhoods
```{r}
d=data%>%
  group_by(neighbourhood)%>%
  summarise(count=n(), mean_price=mean(price))
avg_price=d[order(-d$count),]
avg_price=avg_price[1:5,]
avg_price
ggplot(avg_price, aes(x=neighbourhood, y=count))+geom_bar(stat="identity", aes(fill=neighbourhood))+theme_minimal()+xlab(label="Neighbourhood Type")+ylab("Count")+ggtitle(label="Frequency of Neighbourhood Type")
ggplot(avg_price, aes(x=neighbourhood, y=mean_price))+geom_bar(stat="identity", aes(fill=neighbourhood))+theme_minimal()+xlab(label="Neighbourhood Type")+ylab("Avg price")+ggtitle(label = "Avg Price of most frequent Neighbourhood types")

exp_neigh=d[order(-d$mean_price),]
exp_neigh=exp_neigh[1:5,]
exp_neigh
ggplot(exp_neigh, aes(x=neighbourhood, y=count))+geom_bar(stat="identity", aes(fill=neighbourhood))+theme_minimal()+xlab(label="Neighbourhood Type")+ylab("Count")+ggtitle(label="Frequency of most expensive Neighbourhood Type")

ggplot(exp_neigh, aes(x=neighbourhood, y=mean_price))+geom_bar(stat="identity", aes(fill=neighbourhood))+theme_minimal()+xlab(label="Neighbourhood Type")+ylab("Avg price")+ggtitle(label = "Avg Price of most expensive Neighbourhood types")

```

##### We can see that some of the most expensive neighbourhoods have very few listings. Even though their price is around $600-$800, the avg number of listings is just around 150.
##### Instead, we can concentrate on increasing the price of frequently visited neighbourhoods such as Williamsburg which have number of listings around 4000.



#### Relative Frequency of Neighbourhood groups
```{r}
d=table(data$neighbourhood_group)
d=d/nrow(data)
d
barplot(d, main="Relative Frequency of Neighbourhood group", col = "light blue")
pie(d)
```

##### Some of the most frquently visited neighbourhood groups are Brooklyn and Manhattan and hence requires more focus to increase profits.

#### Finding the statistics of prices in different neighbourhoods
```{r}

levels(data$neighbourhood_group)
p1=data[data$neighbourhood_group=='Brooklyn','price']
p2=data[data$neighbourhood_group=='Manhattan','price']
p3=data[data$neighbourhood_group=='Queens','price']
p4=data[data$neighbourhood_group=='Staten Island','price']
p5=data[data$neighbourhood_group=='Bronx','price']

neigh_1=summary(p1)
neigh_2=summary(p2)
neigh_3=summary(p3)
neigh_4=summary(p4)
neigh_5=summary(p5)

neigh_prices=rbind(neigh_1, neigh_2, neigh_3, neigh_4, neigh_5)
row.names(neigh_prices)<-c('Brooklyn', 'Manhattan', 'Queens', 'Staten Island', 'Bronx')
neigh_prices

```

#### Visualising the price wrt Neighbourhood group
```{r}
ggplot(data, aes(x=neighbourhood_group, y=price))+geom_boxplot(varwidth=T, fill="steelblue")
```

#### Visualising the Price for top host ids of different neighbourhood groups
```{r}
d=data%>%
  filter(price<400)
ggplot(d, aes(x=number_of_reviews))+geom_histogram(bins=70, size=2)+facet_wrap(~neighbourhood_group)+theme_minimal()

```

#### Analyzing the Number of Reviews for each Neighbourhood 
```{r}
d=data%>%
  group_by(neighbourhood)%>%
  filter(reviews_per_month>1)%>%
  summarize(mean_reviews=round(mean(number_of_reviews),2),mean_reviews_per_month=round(mean(reviews_per_month),2))
d

orange_pal <- function(x) rgb(colorRamp(c("#ffe4cc", "#ff9500"))(x), maxColorValue = 255)
green_pal=function(x)rgb(colorRamp(c("#c8e882", "#22a133"))(x),maxColorValue = 255)

reactable(d, columns = list(
  `mean_reviews`=colDef(
    style = function(value){
    normalized=(value-min(d$`mean_reviews`))/(max(d$mean_reviews)-min(d$`mean_reviews`))
    color=orange_pal(normalized)
    list(background=color)
  }),
  mean_reviews_per_month=colDef(
    style=function(value){
      normalized=(value-min(d$mean_reviews_per_month))/(max(d$mean_reviews_per_month)-min(d$mean_reviews_per_month))
      color=green_pal(normalized)
      list(background=color)
    }
  )
),
bordered = TRUE)
```


##### The gradient of the cell is directly proportional to the value.

#### Analysis on the Room type
```{r}
levels(data$room_type)
d=data%>%
  group_by(room_type)%>%
  filter(availability_365!=0)%>%
  select(room_type, availability_365, neighbourhood_group)
d=head(d,50)

ggplot(data, aes(x=room_type))+geom_bar(fill=c("blue", "red", "green"))


formattable(d, list(
  availability_365=color_tile("white", "red")
))

d=data%>%
  group_by(room_type, neighbourhood_group)%>%
  summarise(avg=mean(availability_365))
ggplot(d, aes(x=room_type, y=avg, fill=room_type))+geom_bar(stat="identity")+facet_wrap(~neighbourhood_group)+theme_minimal()


room_price=data%>%
  group_by(room_type)%>%
  summarise(avg_price=round(mean(price),2))

ggplot(room_price, aes(x=room_type, y=avg_price))+geom_bar(stat="identity", fill=c("green", "red", "yellow"))+geom_text(label=room_price$avg_price)+theme_minimal()+xlab("Room Type")+ylab("Average Price")

```

##### We can notice that the most frequently visited rooms are Entire Home/Apt overall. However this varies from neighbourhood to neighbourhood. Based on the visualisation, we can find which type of room requires more focus to maximise profits.

#### Checking the relation between Price above 1000 and Availability
```{r}
d=data%>%
  filter(price>1000 & availability_365!=0)
ggplot(d, aes(x=price, y=availability_365, col=room_type))+geom_point()+xlab("Price")+ylab("Availability in 365 days")
```

##### Most of the room types are within the range 0-2500. However most of the expensive rooms are Entire Home/Apt with availability of all year round. 

```{r}
ggplot(data,aes(x=price, y=number_of_reviews))+geom_point(color="orange")+xlab("Price")+ylab("Number of Reviews")+theme_minimal()

d=data%>%
  filter(price<1000)
ggplot(d,aes(x=price, y=number_of_reviews))+geom_point(color="orange")+xlab("Price")+ylab("Number of Reviews")+theme_minimal()
```

##### With the high number of reviews for price range 0-250, we can assume that it is one of the highly sought after price range by customers. 


## REGRESSION

```{r}
data=read.csv("AB_NYC_2019.csv")
df <- data %>% select(price, # place target variable first
                    #id,
                    #name,                          
                    #host_id,
                    #host_name,
                    neighbourhood_group,
                    #neighbourhood,                 
                    latitude, 
                    longitude,                     
                    room_type,                                               
                    minimum_nights, 
                    number_of_reviews,             
                    #last_review, 
                    reviews_per_month,             
                    calculated_host_listings_count,
                    availability_365)

df$reviews_per_month[is.na(df$reviews_per_month)] <- 0
columns <- c("neighbourhood_group","room_type")
df[, columns] <- df %>% select(all_of(columns)) %>% lapply(as.factor)
df%>%glimpse()
```
#### Outlier Detection and Removal
```{r}

skewness(df$price) # > 1 = highly skewed towards the right
# data with outliers removed
a <- df %>% filter(price < 1000)
skewness(a$price) # > 1 but much less skewed than before
 
```

```{r}
par(mfrow=c(1,2))
qqnorm(df$price); qqline(df$price)
qqnorm(a$price); qqline(a$price)
```

```{r}
Q <- quantile(df$price, probs=c(.25, .75), na.rm = T)
iqr <- IQR(df$price, na.rm = T)
df2 <- df %>% filter(price > (Q[1] - 1.5*iqr) & 
                       price < (Q[2] + 1.5*iqr)) 
```

```{r}
par(mfrow=c(2,1))
options(repr.plot.width=12, repr.plot.height=6)
boxplot(df$price, col = "grey40", horizontal = T, 
        main = "Price - Before Removing Outliers")
boxplot(df2$price, col = "palegreen3", horizontal = T, 
        main = "Price - After Removing Outliers")
```


#### Performing ANOVA
```{r}
aov.ng <- aov(price ~ neighbourhood_group, data = df2)
summary(aov.ng)

```

```{r}
aov.rt <- aov(price ~ room_type, data = df2)
summary(aov.rt)
```

#### Performing TukeyHSD
```{r}
TukeyHSD(aov.ng, which = "neighbourhood_group")
TukeyHSD(aov.rt, which = "room_type")
```

#### Multicollinearity
```{r}
c1 <- df2
cols = c("neighbourhood_group", "room_type")
c1[,cols] <- c1 %>% select_if(is.factor) %>% lapply(as.numeric)


m <- subset(c1, select = -c(price))
mc <- cor(m)

mc
```
#### Checking using VIF
```{r}
fit1 <- lm(price ~., data = c1)
vif(fit1)
```

##### Since the VIF values are lesser than 5, we needn't drop any variables as of now.

#### Splitting Dataset
```{r}
set.seed(123)
split = sample.split(df2$price, SplitRatio = 0.75)
training_set = subset(df2, split == TRUE)
test_set = subset(df2, split == FALSE)
```

#### Model 1 (including outliers)
```{r}
#Backward Elimination
library(SignifReg)

fit1=lm(price~., data=c1)
SignifReg(fit1, scope=training_set,criterion="p-value", direction="backward",alpha=0.05)

m1 <- training_set %>% select(price, # place target variable first
                              neighbourhood_group,
                              latitude, 
                              longitude,                     
                              room_type,                                    
                              minimum_nights, 
                              number_of_reviews,             
                              #reviews_per_month,             
                              calculated_host_listings_count,
                              availability_365
) %>% lm()  
summary(m1)
```

##### On performing backward elimination wrt P-value, we can drop the insignificant variables to maximise the accuracy.

#### Model 2
```{r}
n = ncol(training_set)
fit <- regsubsets(price ~ ., data = training_set, nvmax = n)
sfit <- summary(fit)

# best model has the largest adjutsed R-Squred
which.max(sfit$adjr2) 
coef(fit, which.max(sfit$adjr2))

m2 <- lm(price ~
           neighbourhood_group +
           latitude +
           longitude +
           room_type +
           minimum_nights +
           availability_365,
         data = training_set)
summary(m2)
```
#### Model 3
```{r}
m3 <- lm(price ~
           room_type +
           minimum_nights +
           availability_365 + 
           neighbourhood_group * latitude + 
           room_type * longitude,
         data = training_set)
summary(m3)
```

#### Model Comparison
```{r}
cat("#####   Model Comparison   #####\n");
cat("\nModel 1 R-Squared = ", round(as.numeric(summary(m1)[9]),4))
cat("\nModel 2 R-Squared = ", round(as.numeric(summary(m2)[9]),4))
cat("\nModel 3 R-Squared = ", round(as.numeric(summary(m3)[9]),4))
```

#### Decision Tree Regression
```{r}
library(rpart)
m4 = rpart(formula = price ~ .,
           data = training_set,
           method = "anova")

#Complexity Table
printcp(m4)
caret::varImp(m4)


```

##### Complexity tables help to view the number of splits required to reduce the standard error.

#### Random Forest Regression
```{r}
# Random Forest Regression
library(randomForest)
set.seed(1234)
m6 = rpart(formula = price ~ .,
           data = training_set,
           method = "anova")

printcp(m6)
caret::varImp(m6)
```

```{r}
library(rpart.plot)
rpart.plot(m4, main="Model 4: Decision Tree")
rpart.plot(m6, main="Model 6: Random Forest")
```


#### Support Vector Machine (Takes time)

```{r}
# m7 = svm(formula = price ~ .,
 #               data = training_set,
  #              type = 'eps-regression',
   #             kernel = 'linear')

#summary(m7)

# gamma: 0.07, epsilon:0.1

```

#### Tuning the model
```{r}

# gamma as a sequence = seq(0.001, 1, length = 10)
# t1 <- tune(svm, price ~ ., data = s1, 
  #         ranges = list(gamma = seq(0.001, 1, length = 10), 
   #                      cost = 2^(2:4)),
    #       tunecontrol = tune.control(sampling = "fix"))

# t1$best.model
# gamma: 0.5, epsilon: 0.1, cost: 4
```

##### Model tuning is done to find the optimal hyperparameters to increase the accuracy.

#### Applying the tuning
```{r}
m7 = svm(formula = price ~ .,
         data = training_set,
         type = 'eps-regression',
         kernel = 'radial',
         gamma = 0.5,
         cost = 4)
summary(m7)
```


### Prediction
```{r}
y_pred1 = predict(m1, newdata = test_set)
y_pred2 = predict(m2, newdata = test_set) 
y_pred3 = predict(m3, newdata = test_set)
y_pred4 = predict(m4, newdata = test_set) 
y_pred6 = predict(m6, newdata = test_set)
y_pred7 = predict(m7, newdata = test_set)
```

#### Model Comparison using Accuracy
```{r}
#------ model 1  --------
MSE1 = sum((y_pred1 - test_set$price)^2)/nrow(test_set)
var.y1 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr1 = 1 - (MSE1/var.y1)

#------ model 2  --------
MSE2 = sum((y_pred2 - test_set$price)^2)/nrow(test_set)
var.y2 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr2 = 1 - (MSE2/var.y2)

#------ model 3  --------
MSE3 = sum((y_pred3 - test_set$price)^2)/nrow(test_set)
var.y3 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr3 = 1 - (MSE3/var.y3)

#------ model 4  --------
MSE4 = sum((y_pred4 - test_set$price)^2)/nrow(test_set)
var.y4 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr4 = 1 - (MSE4/var.y4)

#------ model 6  --------
MSE6 = sum((y_pred6 - test_set$price)^2)/nrow(test_set)
var.y6 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr6 = 1 - (MSE6/var.y6)

#------ model 7  --------
MSE7 = sum((y_pred7 - test_set$price)^2)/nrow(test_set)
var.y7 = sum((test_set$price - mean(test_set$price))^2)/(nrow(test_set)-1)
Rsqr7 = 1 - (MSE7/var.y7)
```

```{r}
cat("#####     Compare Model Accuracy     #####\n")
cat("\nMSE Model 1 = ", MSE1, " Variance  = ", var.y1, "R Squared = ", Rsqr1)
cat("\nMSE Model 2 = ", MSE2, " Variance  = ", var.y2, "R Squared = ", Rsqr2)
cat("\nMSE Model 3 = ", MSE3, " Variance  = ", var.y3, "R Squared = ", Rsqr3)
cat("\nMSE Model 4 = ", MSE4, " Variance  = ", var.y4, "R Squared = ", Rsqr4)
cat("\nMSE Model 6 = ", MSE6, " Variance  = ", var.y6, "R Squared = ", Rsqr6)
cat("\nMSE Model 7 = ", MSE7, " Variance  = ", var.y7, "R Squared = ", Rsqr7)
```

#### On viewing the accuracy, we can see that SVM along with Model Tuning has the highest accuracy and thus makes the best model out of all the other implemented models.